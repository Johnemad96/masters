| Algorithm | Pros | Cons | Suitable for | Open Source Availability | Possible Implementation Link | Difficulty to Implement (1-10) | Average Complexity Analysis | Suitability for Your Case (1-10) | How it works |
|-----------|------|------|--------------|-------------------------|-------------------------------|-----------------------------|-----------------------------|----------------------------------|--------------|
| Grid Search | Simple to understand and implement. Exhaustive search ensures that the best parameters within the grid will be found. | Computationally expensive. Not efficient for large number of parameters. | Small number of parameters | Yes | [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) | 1 | O(n^d) where n is the number of grid points for one parameter and d is the number of parameters | 8 | Grid Search works by exhaustively trying every combination of parameters in a predefined grid. |
| Random Search | More efficient than grid search for large number of parameters. Easy to implement. | No guarantee to find the best parameters. | Both small and large number of parameters | Yes | [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) | 1 | O(n) where n is the number of iterations | 7 | Random Search works by randomly selecting combinations of parameters from a predefined grid or distribution. |
| Genetic Algorithm (GA) | Can handle complex search spaces. Good balance between exploration and exploitation. | More complex to implement and understand. May converge to local optima. | Both small and large number of parameters | Yes | [DEAP](https://github.com/DEAP/deap) | 6 | O(g*n*log(n)) where g is the number of generations and n is the population size | 6 | Genetic Algorithms work by simulating the process of natural selection, where the fittest individuals are selected for reproduction to produce the offspring of the next generation. |
| Whale Optimization Algorithm (WOA) | Can handle complex search spaces. Good balance between exploration and exploitation. | More complex to implement and understand. Less mature and tested than other methods. | Both small and large number of parameters | Yes | [WOA](https://github.com/7ossam81/EvoloPy) | 7 | O(n) where n is the number of iterations | 5 | Whale Optimization Algorithm simulates the social behavior of humpback whales. The algorithm is based on the bubble-net hunting strategy. |
| Bayesian Optimization | Can find better solutions with fewer evaluations. Good for expensive evaluation functions. | More complex to implement and understand. Requires choosing a surrogate model and an acquisition function. | Both small and large number of parameters | Yes | [Hyperopt](http://hyperopt.github.io/hyperopt/), [Spearmint](https://github.com/JasperSnoek/spearmint) | 8 | O(n) where n is the number of iterations | 6 | Bayesian Optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which ones are not. |
| Tree-structured Parzen Estimator (TPE) | Can handle both continuous and discrete parameters. Good for expensive evaluation functions. | More complex to implement and understand. | Both small and large number of parameters | Yes | [Hyperopt](http://hyperopt.github.io/hyperopt/) | 8 | O(n) where n is the number of iterations | 6 | TPE models P(x\|y) and P(y) where x is hyperparameters and y is the associated loss. P(x\|y) is modeled by transforming the generative process of hyperparameters, replacing the distributions of the configuration prior with non-parametric densities. |
| Particle Swarm Optimization (PSO) | Can handle complex search spaces. Good balance between exploration and exploitation. | More complex to implement and understand. May converge to local optima. | Both small and large number of parameters | Yes | [pyswarm](https://github.com/tisimst/pyswarm) | 7 | O(n) where n is the number of iterations | 5 | PSO is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. |
| Simulated Annealing (SA) | Can handle complex search spaces. Can escape local optima. | More complex to implement and understand. Performance depends on the cooling schedule. | Both small and large number of parameters | Yes | [simanneal](https://github.com/perrygeo/simanneal) | 6 | O(n) where n is the number of iterations | 5 | Simulated Annealing is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). |
| CMA-ES | Handles noisy and non-convex functions. Invariant to linear rescalings. | More complex to implement and understand. | Both small and large number of parameters | Yes | [cma-es](https://github.com/CMA-ES/pycma) | 7 | O(n^2) to O(n^3), where n is the number of parameters | 6 | CMA-ES is an evolutionary algorithm for difficult non-linear non-convex optimization problems in continuous domain. The algorithm treats the parameters of the optimization problem as a multivariate distribution and uses the distribution's covariance matrix to adaptively adjust the search. |
| Differential Evolution (DE) | Simple yet powerful. Good for high-dimensional problems. | May converge to local optima. | Both small and large number of parameters | Yes | [SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html) | 4 | O(n) where n is the number of iterations | 6 | Differential Evolution is a stochastic, population-based algorithm that is used for global optimization problems. At each step, it mutates the population by adding the weighted difference between two individuals to a third individual. It then uses crossover to combine the mutant with another, unmutated individual to create a trial individual. If the trial is better than the unmutated individual, it replaces it. |
| Nelder-Mead Method | Does not require gradient information. | May converge slowly. Not suitable for non-smooth functions. | Small number of parameters | Yes | [SciPy](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html) | 3 | O(n^2) where n is the number of parameters | 5 | The Nelder-Mead method is a direct search method (does not use gradient information) and is often applied to nonlinear optimization problems for which derivatives may not be known. It uses the concept of a simplex, which is a polytope of n + 1 vertices in n dimensions, and then reflects, expands, contracts, and shrinks the simplex to find the minimum. |
| Gradient Descent | Simple to understand and implement. | Requires gradient information. May converge to local optima. | Both small and large number of parameters | Yes | [SciPy](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-gradientdescent.html) | 2 | O(n) where n is the number of iterations | 4 | Gradient Descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. |
| Newton's Method | Can find exact solution in one step for quadratic problems. | Requires second order derivative information. May not converge for non-convex functions. | Small number of parameters | Yes | [SciPy](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html) | 5 | O(n^2) where n is the number of parameters | 4 | Newton's Method is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function. It can also be used for minimizing a function, by finding stationary points (where the derivative is zero). |
